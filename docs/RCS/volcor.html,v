head	1.4;
access;
symbols;
locks;
comment	@# @;


1.4
date	2013.08.28.16.30.19;	author leith;	state Exp;
branches;
next	1.3;

1.3
date	2006.09.13.12.40.51;	author leith;	state Exp;
branches;
next	1.2;

1.2
date	2004.07.29.14.09.59;	author leith;	state Exp;
branches;
next	1.1;

1.1
date	2004.04.26.13.48.05;	author leith;	state Exp;
branches;
next	;


desc
@new
@


1.4
log
@*** empty log message ***
@
text
@<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2//EN">
<html>
<head>
<meta name="generator" content="HTML Tidy, see www.w3.org">
<title>Volume Correlation</title>
</head>
<body>
<h2 align="center">Volume Correlation</h2>

This document is to serve as the how-to-use file for the group of
SPIDER procedure files pertaining to volume correlation. This
includes VolCorr.bat, VCWH.bat, and hc.bat. This document is
arranged as follows: 

<ol>
<li>User level documentation - how to use this group of programs on
a daily basis.</li>

<li>General Explanation - theory on partitions and why things are
why they are</li>

<li>Programmer level documentation - in depth explanation of code,
for ease of modification</li>

<li>Acknowledgements</li>
</ol>

<ol>
<li>User level documentation OR: How I Learned to Stop Worrying And
Use 688 Lines Of Code</li>

<li style="list-style: none">
<ul>
<li style="list-style: none">
<p></p>
</li>

<li>Purpose<br>
 The purpose of VolCorr and VCWH is to create control points to
align two 3D double-tilt reconstructions of the same object created
by SPIDER. Often, reconstructions of the same object after a 90
degree rotation have noticeable differences in the location of
organelles, regions, etc. that can not be explained. This group of
programs select regions in one reconstruction, and then search for
and return the placement of the best fit of the region in the
second reconstruction. The position of the searched for regions in
each volume is output as a document file, VCOutput.bat. At the time
of writing this, modification of a warp program is underway to
perform the actual warping, using this information.</li>

<li>Assumptions/Caveats/Warnings<br>
 

<ol>
<li>The two reconstructions(volumes) to be compared must be of the
same size. This should not be a problem considering the two volumes
are of the same object.</li>

<li>The volumes should be aligned in the same direction. There is a
built in limitation of how far the program will search for the best
fit region in the second volume. This limit is based on how many
partitions are created(see 2.1 and 3.0 326-58). Alignment should be
based on real life arrangement of the sample.</li>

<li>Only translation is accounted for. If you have reason to
believe that there is large scale rotation BETWEEN your volumes,
consult Bimal's paper/program(see 4.0).</li>

<li>This program searches and correlates REGIONS of space. This
takes into accounts specific organelles, local regions, as well as
background density. It selects the search region from the first
volume, and finds the best fit of the entire region, within a
certain distance. If you are trying to find certain objects or
organelles, look at Bimal's paper/program(see 4.0).</li>

<li>That all three files and test data are in the same folder. If
not, you may have to type in file names accordingly different.</li>

<li>Any other applicable limitations either of Bimal's work or
Roseman's paper (see 4.0).</li>
</ol>

<br>
<br>
</li>

<li>Directions For Daily Use<br>
 What follows is a list of general steps on how to manipulate a
reconstruction, to produce the best correlations and why. It is
based only on my experience and therefore may include extra, or
accidentally omit, steps that you might find useful. The entire
purpose of steps (1.3.1 to 1.3.6)is to align the volumes correctly
and remove random noise in each reconstruction. 

<ol>
<li>Obtain two(or more) 3D reconstructions of the same sample.
There is no reason that I know of why they could not be single
tilt, other than they contain less information.</li>

<li>View the volumes, in their entirity, either with WEB or some
other program. The purpose of this is to insure that the volumes
are aligned in the same manner. If not, use RT 90 in SPIDER to
align if at all possible. RT 90 preserves more information than
other rotate commands.</li>

<li>After you are sure that the volumes are aligned properly, use
the Commands-&gt;Histogram option in WEB to view the histogram of
each volume. Take note of the PERCENTAGE where the bell curve shape
become too elongated on either end for each volume. It should be
roughly the same for each. In my experience, the gold specks that
are used in the samples drastically skew the histogram tails
outward. This forces a limited number of grey scales to represent
the full spread of density values. Resulting in the usable sample
information contained in a very small amount of the grey scale, so
that you can not see where the "noise" ends and the true
information begins. This knowledge is used later.</li>

<li>With the percentage values required to make each volume's
histogram more "bell-curvy" open hc.bat.</li>

<li>Change the name directly after[infile] to the full path name
needed to find the first volume.</li>

<li>Change after [outfile] to the name and location of where the
"compressed" image will be made. This needs to be a different name
than the source because a)SPIDER won't rewrite the source and b)the
source will be used later.</li>

<li>Change the values of x50,x51 to that the needed % values are in
DECIMAL form, 58% = 0.58</li>

<li>Run hc.bat from SPIDER</li>

<li>View the new volumes in WEB. Select the points(slices) where
the noise stops, and useful information begins. It is my experince
that the most noise only occurs in the top or bottom slices, the
sides are relatively usefull. If this is not the case for you,
remove ANY large amounts of noise. This program is not designed to
compare random noise. The amount of noisy slices removed from each
volume should be equal. Equal because VolCorr requires volumes to
be the same size, and if you remove uneven amounts, but still have
the same size volumes, you are no longer comparing pixels
correctly. This also typically removes the gold specks because they
are usually found only in the noise region. A slight loss of useful
data is allowable in order to remove more extreme data(gold specks
and shadows).</li>

<li>Using the SPIDER WIndow command, select the correct slices from
the ORIGINAL (aligned, but not compressed) volumes, and save as new
files. These new files are what will be compared. The reason you
remove from the aligned, but not compressed file, is that you
compare pure data. No noise AND no artifacts from
thresholding.</li>

<li>Because VolCorr is a procedure file, you must alter the file
itself, no arguments allowed. In order to make that safer, ONLY
CHANGE THINGS BETWEEN THE ----ALL INPUTS--- and ---END ALL
INPUTS--- lines.</li>

<li>The name/path after [secondary] is the file that the
searched-for regions will come from.</li>

<li>After [primary] is the file that the regions will be
searched-in for. Which file is first or second is important because
the output of VolCorr is where the points in [secondary] can be
found in [primary]. This determines if you want to warp file 2 to
look like file 1, or vise versa.</li>

<li>The value after "x91 =" sets the minimum value for a pixel to
be searched for. For example, if you run SPIDER's FS and set the
"x91 =" value to the maximum valueof a volume, then no pixels will
be searched for. A good rule of thumb is around 10% of the
difference between the maximum and minimum. This value has a large
effect on correlations, so if you're having problems forming
reasonable correlations, examine this value.</li>

<li>The next three variables determine the number of search
regions. The number of divisions(regions) in a dimension determines
how many spaces it creates in that dimension. Thus, all three
multiplied together are how many regions there are in all. See Sec.
2.1 for more in depth explanation.</li>

<li>The last three are used to determine the size of each search
region. Similar to the number of regions, multiply these values
together to find the volume of each region.</li>
</ol>
</li>
</ul>

<br>
<br>
</li>

<li>General Explanation - Partitions, Search Regions, and Jump
Values (Oh My!)<br>
 

<ol>
<li>After many iterations of this program, I realized that a
point-centric model was the most efficient way to determine and
maintain the multiple spaces that are needed. The term
point-centric pertains to the method of creating and locating the
search regions in their respective partitions. The method
determines the center of a search region or partition, and then
expands from this central point, to create the space. This is
instead of SPIDER's inherent system of defining the upper-left
corner, then expanding from that. For variables x77-9 the term
divisions is used. The easiest way of visualizing divisions is to
imagine a cake. Each variable describes how many divisions in a
particular dimension will be made. If x77 = 3, then there will be
two cuts made. Resulting in three slices, or divisions, of cake in
the X direction. This same thought process can be used for Y and Z
dimensions as well. It follows that if there is 3,5,and 7 divisions
in the X,Y, and Z dimensions that there will be 3*5*7=105 pieces of
cake, or partitions. In order for the point-centric model to be
useful, there must be one point, or voxel, that is the center of
each partition. To insure this, there are tests to force each
division, and therefor each partition, height to be an odd number.
This central voxel of the partition is also the center point of a
search region. Using the variables that are responsible for "cube
size", the search cube is developed by expanding out from the
center voxel. If x80 = 11, the X size of the search cube is
(central point's x value minus five) to (central point's x value
plus five). In this way, the point-centric method forces the search
cube, and it's respective partition, to be co-centric.</li>

<li>With the coordinates of the search region defined, it is copied
out from the secondary volume and transformed into Fourier space.
Then the respective partition is copied out of the primary volume
and also changed into Fourier space. They are then compared to each
other, and the distance between their best fit's are calculated.
This distance is returnedas the output. A simple test is to compare
a file to itself. The two points should be identical, as long as
there is a minimum of white noise, and therefore a distnce of 0
returned.</li>
</ol>
</li>

<li>Programmer level documentation 

<p>What follows is a VERY rough, line-by-line account of what is
going on, and why.</p>

<p>VolCorr.bat</p>

<ul>
<li>42-85 The point-centric method requires everything to be odd,
for one central voxel</li>

<li>84-119 One of the primary assumptions is that the files are
aligned correctly.</li>

<li>121-135 Point centric: partition height = volume height/ # of
divisions wanted, then make sure it's odd</li>

<li>137-140 Calculate the center of the partition: partition
height/2 the 0.5 insures the center</li>

<li>142-171 Insure that the search cube isn't larger than it's
partition</li>

<li>173-176 For print out, just to know how much of the volume is
included in the search cubes.</li>

<li>178-181 Space between divisions:= (volume height - total
partition height)/# of spaces between</li>

<li>219 Start VolCorrWorkHorse, see top of VolCorr for variable
explanation</li>
</ul>

<br>
<br>
 VCWH.bat<br>
WARNING due to the many incarnations of this program, some comments
in VCWH may refer to similar things with different terminology than
used in this ReadMe. The process is correct, but the objects
affected may be different than how I describe them here. I wrote
this ReadMe to be the diffinitive explanation, it SHOULD supersede
any description in VCWH, but "caveat programmtor". 

<p></p>

<ul>
<li>12-14 CC stores results in the center of the image, need upper
left of center window</li>

<li>21-23 see 2.1, this is 0.5 of the search cube size, used to
"expand out"</li>

<li>26-43 similar to 142-171 above</li>

<li>48-63 write to results file info needed to display
movement</li>

<li style="list-style: none">vectors</li>

<li>67-72 one loop needed for each partition, or part of cake</li>

<li>75-77 for progress display purposes</li>

<li>79-87 using loop counters, selects the partition that will be
searched in, from primary = # of divisions-1* size of partitions +
space between partitions*spaces between</li>

<li>89-92 location of corresponding search cube =corner of
partition +center of partition- half the size of search cube</li>

<li>94-99 to change print out to final points instead of vectors,
x55=x52+x58</li>

<li>101-106 select partition, from primary</li>

<li>108-125 the actual theory is best described by others, see
Roseman's paper</li>

<li>127-131 WU=local.std.dev. returns in center, like CC, so need
upper left</li>

<li>133-137 before CC, must center the search cube in the
partition</li>

<li>139-144 select search cube from secondary</li>

<li>146-324 see Roseman's paper</li>

<li>326-358 this is what actually limits max. distance the search
cube can be from the expected position</li>

<li>365-372 looking for the highest points of correlation</li>

<li>374-391 Prints out only the highest point of correlation for
each partition/search cube. If more correlations are desired, place
a mini-do loop around all, and change the "1" from UD S to the loop
counter.</li>

<li>393-end file clean up, end of XYZ do loops, doesn't actually
delete output dir. Feel free to fix that.</li>
</ul>

<br>
<br>
</li>

<li>Acknowledgements - Requiem For A ReadMe<br>
 

<p>Heartfelt thanks to Dr. Joachim Frank and Dr. ArDean Leith, for
giving me the chance to prove my mettle. With their help, I have
proven how well a bioengineer with 3 programming classes can
code.</p>

<p>Alan Roseman whose paper found in Ultramicroscopy, Vol 94,
Issues 3-4, (2003), 225-236) first developed and proved the
efficacy of the algorithm.</p>

<p>Bimal Rath, who first developed Roseman's paper into a viable
SPIDER procedure file.  His program is discussed 
<a href="./partpick.html">here</a>.</p>

<p>VolCorr, VCWH, hc, and this ReadMe written by Jamie LeBarron.
Please keep in mind, bad jokes are better than none at all. In
regards to the length, I rated usefullness over compactness.
12/29/03</p>
</li>
</ol>
</body>
</html>

@


1.3
log
@procedure
@
text
@d358 2
a359 2
SPIDER procedure file. His program is discussed at
http://www.wadsworth.org/spider_doc/spider/docs/partpick.htm</p>
@


1.2
log
@xhtml
@
text
@d155 1
a155 1
<li>Because VolCorr is a batch file, you must alter the file
d358 1
a358 1
SPIDER batch file. His program is discussed at
@


1.1
log
@Initial revision
@
text
@d1 1
a1 1

d4 1
d7 2
a8 2

</body>
a9 1
<h2 align="center"> Volume Correlation</h2>
d11 3
a13 3
SPIDER procedure files pertaining to volume correlation. This includes
VolCorr.bat, VCWH.bat, and hc.bat. This document is arranged as
follows:
d16 5
a20 5
<p>
<li> User level documentation - how to use this group of programs on a
daily basis.</li>
<li>General Explanation - theory on partitions and why things are why
they are</li>
d22 2
a23 2
<li> Programmer level documentation - in depth explanation of code, for
ease of modification</li>
d25 1
a25 2
<li> Acknowledgements</li>
</p>
d28 3
a30 3
<ol> 
<li>User level documentation OR: How I Learned to Stop Worrying
     And Use 688 Lines Of Code</li>
d32 1
d34 52
a85 136
   <p>
   <li> Purpose<br>
   The purpose of VolCorr and VCWH is to create control points to align
   two 3D double-tilt reconstructions of the same object created by
   SPIDER.  Often, reconstructions of the same object after a 90 degree
   rotation have noticeable differences in the location of organelles,
   regions, etc. that can not be explained. This group of programs select
   regions in one reconstruction, and then search for and return the
   placement of the best fit of the region in the second reconstruction.
   The position of the searched for regions in each volume is output as a
   document file, VCOutput.bat. At the time of writing this, modification
   of a warp program is underway to perform the actual warping, using this
   information.</li></p>

   <li> Assumptions/Caveats/Warnings<br> 
      <ol> 
      <li>The two
      reconstructions(volumes) to be compared must be of the same size. This
      should not be a problem considering the two volumes are of the same
      object.</li>

      <li>The volumes should be aligned in the same direction. There is a
      built in limitation of how far the program will search for the best fit
      region in the second volume. This limit is based on how many partitions
      are created(see 2.1 and 3.0 326-58). Alignment should be based on real
      life arrangement of the sample.</li>

      <li>Only translation is accounted for. If you have reason to believe
      that there is large scale rotation BETWEEN your volumes, consult
      Bimal's paper/program(see 4.0).</li>

      <li>This program searches and correlates REGIONS of space. This takes
      into accounts specific organelles, local regions, as well as background
      density. It selects the search region from the first volume, and finds
      the best fit of the entire region, within a certain distance. If you
      are trying to find certain objects or organelles, look at Bimal's
      paper/program(see 4.0).</li>

      <li>That all three files and test data are in the same folder. If not,
      you may have to type in file names accordingly different.</li>

      <li>Any other applicable limitations either of Bimal's work or
      Roseman's paper (see 4.0).</li>
        </ol>
        </p>

   <li> Directions For Daily Use<br>
      What follows is a list of general steps on how to manipulate a reconstruction, to produce the best correlations and why. It is based only on my experience and therefore may include extra, or accidentally omit, steps that you might find useful. The entire purpose of steps (1.3.1 to 1.3.6)is to align the volumes correctly and remove  random noise in each reconstruction. 

      <ol>
      <li>Obtain two(or more) 3D reconstructions of the same sample. 
      There is no reason that I know of why they could not be single tilt, 
      other than they contain less information.</li>

      <li>View the volumes, in their entirity, either with WEB or some other
      program. The purpose of this is to insure that the volumes are aligned
      in the same manner. If not, use RT 90 in SPIDER to align if at all
      possible. RT 90 preserves more information than other rotate commands.</li>

      <li>After you are sure that the volumes are aligned properly, use the
      Commands->Histogram option in WEB to view the histogram of each volume.
      Take note of the PERCENTAGE where the bell curve shape become too
      elongated on either end for each volume. It should be roughly the same
      for each. In my experience, the gold specks that are used in the
      samples drastically skew the histogram tails outward. This forces a
      limited number of grey scales to represent the full spread of density
      values. Resulting in the usable sample information contained in a very
      small amount of the grey scale, so that you can not see where the
      "noise" ends and the true information begins. This knowledge is used
      later.</li>

      <li>With the percentage values required to make each volume's
      histogram more "bell-curvy" open hc.bat.</li>

      <li>Change the name directly after[infile] to the full path name
      needed to find the first volume.</li>

      <li> Change after [outfile] to the name and location of where the
      "compressed" image will be made. This needs to be a different name than
      the source because a)SPIDER won't rewrite the source and b)the source
      will be used later.</li>

      <li>Change the values of x50,x51 to that the needed % values are in
      DECIMAL form, 58% = 0.58</li>

      <li>Run hc.bat from SPIDER</li>

      <li>View the new volumes in WEB. Select the points(slices) where the
      noise stops, and useful information begins. It is my experince that the
      most noise only occurs in the top or bottom slices, the sides are
      relatively usefull. If this is not the case for you, remove ANY large
      amounts of noise. This program is not designed to compare random noise.
      The amount of noisy slices removed from each volume should be equal.
      Equal because VolCorr requires volumes to be the same size, and if you
      remove uneven amounts, but still have the same size volumes, you are no
      longer comparing pixels correctly. This also typically removes the gold
      specks because they are usually found only in the noise region. A
      slight loss of useful data is allowable in order to remove more extreme
      data(gold specks and shadows).</li>

      <li>Using the SPIDER WIndow command, select the correct slices from
      the ORIGINAL (aligned, but not compressed) volumes, and save as new
      files. These new files are what will be compared. The reason you remove
      from the aligned, but not compressed file, is that you compare pure
      data. No noise AND no artifacts from thresholding.</li>

      <li>Because VolCorr is a batch file, you must alter the file itself,
      no arguments allowed. In order to make that safer, ONLY CHANGE THINGS
      BETWEEN THE ----ALL INPUTS--- and ---END ALL INPUTS--- lines.</li>

      <li>The name/path after [secondary] is the file that the searched-for
      regions will come from.</li>

      <li> After [primary] is the file that the regions will be searched-in
      for. Which file is first or second is important because the output of
      VolCorr is where the points in [secondary] can be found in [primary].
      This determines if you want to warp file 2 to look like file 1, or vise
      versa.</li>

      <li>The value after "x91 =" sets the minimum value for a pixel  to be
      searched for. For example, if you run SPIDER's FS and set the "x91 ="
      value to the maximum valueof a volume, then no pixels will be searched
      for. A good rule of thumb is around 10% of the difference between  the
      maximum and minimum. This value has a large effect on correlations, so
      if you're having problems forming reasonable correlations, examine this
      value.</li>

      <li>The next three variables determine the number of search regions.
      The number of divisions(regions) in a dimension determines how many
      spaces it creates in that dimension. Thus, all three multiplied
      together are how many regions there are in all. See Sec. 2.1 for more
      in depth explanation.</li>

      <li>The last three are used to determine the size of each search
      region. Similar to the number of regions, multiply these values
      together to find the volume of each region.</li>
d87 99
d187 1
d190 3
a192 1
</p>
d194 3
a196 1
<li>General Explanation - Partitions, Search Regions, and Jump Values (Oh My!)<br>
d199 1
a199 2

<p><li> After many iterations of this program, I realized that a
d201 36
a236 36
maintain the multiple spaces that are needed. The term point-centric
pertains to the method of creating and locating the search regions in
their respective partitions. The method determines the center of a
search region or partition, and then expands from this central point,
to create the space. This is instead of SPIDER's inherent system of
defining the upper-left corner, then expanding from that.
	For variables x77-9 the term divisions is used. The easiest way
	of visualizing divisions is to imagine a  cake. Each variable
	describes how many divisions in a particular dimension will be
	made. If x77 = 3, then there will be two cuts made. Resulting
	in three slices, or divisions, of cake in the X direction. This
	same thought process can be used for Y and Z dimensions as
	well. It follows that if there is 3,5,and 7 divisions in the
	X,Y, and Z dimensions that there will be 3*5*7=105 pieces of
	cake, or partitions. In order for the point-centric model to be
	useful, there must be one point, or voxel, that is the center
	of each partition. To insure this, there are tests to force
	each division, and therefor each partition, height to be an odd
	number.  This central voxel of the partition is also the center
	point of a search region. Using the variables that are
	responsible for "cube size", the search cube is developed by
	expanding out from the center voxel. If x80 = 11, the X size of
	the search cube is (central point's x value minus five) to
	(central point's x value plus five). In this way, the
	point-centric method forces the search cube, and it's
	respective partition, to be co-centric.</li></p>

<p><li>With the coordinates of the search region defined, it is copied
out from the secondary volume and transformed into Fourier space. Then
the respective partition is copied out of the primary volume and also
changed into Fourier space. They are then compared to each other, and
the distance between their best fit's are calculated. This distance is
returnedas the output.  A simple test is to compare a file to itself.
The two points should be identical, as long as there is a minimum of
white noise, and therefore a distnce of 0 returned.</li></p>
</ul>
d239 1
a239 1
<li>Programmer level documentation
d241 2
a242 2
<p> What follows is a VERY rough, line-by-line account of what is going on,
and why. </p> 
d244 1
a244 1
<p> VolCorr.bat </p> 
d247 2
a248 2
<li>42-85   The point-centric method
requires everything to be odd, for one central voxel</li>
d250 2
a251 1
<li>84-119	One of the primary assumptions is that the files are aligned correctly.</li>
d253 2
a254 1
<li>121-135	Point centric: partition height = volume height/ # of divisions wanted, then make sure it's odd</li>
d256 2
a257 1
<li>137-140	Calculate the center of the partition: partition height/2 the 0.5 insures the center</li>
d259 2
a260 1
<li>142-171	Insure that the search cube isn't larger than it's partition</li>
d262 2
a263 1
<li>173-176	For print out, just to know how much of the volume is included in the search cubes.</li>
d265 2
a266 1
<li>178-181 Space between divisions:= (volume height - total partition height)/# of spaces between</li>
d268 2
a269 2
<li>219	Start VolCorrWorkHorse, see top of VolCorr for variable 
        explanation</li>
d271 10
a280 1
</p>
d282 1
a282 7
VCWH.bat<br>
WARNING due to the many incarnations of this program, some comments in VCWH may refer to 
similar things with different terminology than used in this ReadMe. The process is correct, 
but the objects affected may be different than how I describe them here. I wrote this ReadMe 
to be the diffinitive explanation, it SHOULD supersede any description in VCWH, but "caveat programmtor".
   <p>
   <ul>
d284 6
a289 2
   <li>12-14    CC stores results in the center of the image, need
   upper left of center window</li>
d291 1
a291 2
   <li>21-23    see 2.1, this is 0.5 of the search cube size, used to
   "expand out"</li>
d293 2
a294 1
   <li>26-43    similar to 142-171 above</li>
d296 1
a296 2
   <li>48-63    write to results file info needed to display movement</li>
   vectors
d298 1
a298 1
   <li>67-72    one loop needed for each partition, or part of cake</li>
d300 1
a300 1
   <li>75-77    for progress display purposes</li>
d302 3
a304 3
   <li>79-87    using loop counters, selects the partition that will be
   searched in, from primary = # of divisions-1* size of partitions 
   + space between partitions*spaces between</li>
d306 2
a307 3
   <li>89-92    location of corresponding search cube =corner of 
           partition +center  of partition- half the size of search 
           cube</li>
d309 2
a310 2
   <li>94-99    to change print out to final points instead of vectors,
                  x55=x52+x58</li>
d312 1
a312 1
   <li>101-106  select partition, from primary</li>
d314 2
a315 2
   <li>108-125  the actual theory is best described by others, see
   Roseman's paper
d317 2
a318 2
   <li>127-131  WU=local.std.dev. returns in center, like CC, so need
   upper left
d320 2
a321 1
   <li>133-137  before CC, must center the search cube in the partition</li>
d323 1
a323 1
   <li>139-144  select search cube from secondary</li>
d325 1
a325 1
   <li>146-324  see Roseman's paper</li>
d327 2
a328 2
   <li>326-358  this is what actually limits max. distance the search
   cube can be from the expected position</li>
d330 1
a330 1
   <li>365-372  looking for the highest points of correlation</li>
d332 4
a335 4
   <li>374-391 Prints out only the highest point of correlation for
   each partition/search cube. If more correlations are        desired,
   place a mini-do loop around all, and change the "1" from UD S to the
   loop counter.</li>
d337 3
a339 4
   <li>393-end  file clean up, end of XYZ do loops, doesn't actually
   delete output dir. Feel free to fix that.</li>
   </ul>
   </p>
d341 3
a343 1
<li>Acknowledgements - Requiem For A ReadMe <br>
d345 2
a346 3
<p> Heartfelt thanks to Dr. Joachim Frank and Dr. ArDean Leith, for giving
me the chance to prove my mettle. With their help, I have proven how
well a bioengineer with 3 programming classes can code.</p>
d348 8
a355 3
<p>Alan Roseman whose paper found in Ultramicroscopy, Vol 94, Issues 3-4,
(2003), 225-236) first developed and proved the efficacy of the
algorithm.</p>
d357 2
a358 2
<p>Bimal Rath, who first developed Roseman's paper into a viable SPIDER
batch file. His program is discussed at
d361 4
a364 3
<p>VolCorr, VCWH, hc, and this ReadMe written by Jamie LeBarron. Please
keep in mind, bad jokes are better than none at all. In regards to the
length, I rated usefullness over compactness. 12/29/03</p>
a365 1

a366 1

d369 1
@
